{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36e1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from utils import cv_trainer\n",
    "from base import persian_text_preprocessing\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1762e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"hezarai/sentiment-dksf\")\n",
    "ds = ds.map(lambda example: {**example, 'text': persian_text_preprocessing(example['text'])}, \n",
    "           batched=False)\n",
    "train_df: pd.DataFrame = ds[\"train\"].to_pandas() # type: ignore\n",
    "test_df: pd.DataFrame = ds[\"test\"].to_pandas() # type: ignore\n",
    "df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281f1b6",
   "metadata": {},
   "source": [
    "# Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf1e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings from comments_embeddings.npy\n",
      "Embeddings shape: (30917, 768)\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings_batch(texts, model=\"nomic-embed-text\", batch_size=300):\n",
    "    all_embeddings = []\n",
    "    texts_list = texts if isinstance(texts, list) else texts.to_list()\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts_list), batch_size), desc=\"Creating embeddings\"):\n",
    "        batch = texts_list[i:i + batch_size]\n",
    "        response = ollama.embed(model=model, input=batch)\n",
    "        all_embeddings.extend(response[\"embeddings\"])\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "embeddings_file = './data/comments_embeddings.npy'\n",
    "\n",
    "if os.path.exists(embeddings_file):\n",
    "    print(f\"Loading existing embeddings from {embeddings_file}\")\n",
    "    comments_dense_embeddings = np.load(embeddings_file)\n",
    "else:\n",
    "    print(\"Creating new embeddings...\")\n",
    "    comments_dense_embeddings = create_embeddings_batch(df[\"text\"])\n",
    "    \n",
    "    np.save(embeddings_file, comments_dense_embeddings)\n",
    "    print(f\"Embeddings saved to {embeddings_file}\")\n",
    "\n",
    "print(f\"Embeddings shape: {comments_dense_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444e23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense, y_dense = comments_dense_embeddings, df[\"label\"].to_numpy()\n",
    "\n",
    "train_size = len(train_df)\n",
    "X_train_dense = X_dense[:train_size]\n",
    "X_test_dense = X_dense[train_size:]\n",
    "y_train_dense = y_dense[:train_size]\n",
    "y_test_dense = y_dense[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89ba155",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dense = {\n",
    "     'logistic_regression': (\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "        ]),\n",
    "        {\n",
    "            'scaler': [None, StandardScaler()],\n",
    "            'pca': [None, PCA(50), PCA(100), PCA(200)],\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__solver': ['liblinear', 'lbfgs'],\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'gaussianNB': (\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classifier', GaussianNB())\n",
    "        ]),\n",
    "        {\n",
    "            'scaler': [None, StandardScaler(), MinMaxScaler()],\n",
    "            'pca': [None, PCA(50), PCA(100), PCA(200)],\n",
    "            'classifier__var_smoothing': [1e-5, 1e-7, 1e-9]\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'random_forest': (\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        {\n",
    "            'scaler': [None, StandardScaler()],\n",
    "            'pca': [None, PCA(100), PCA(200)],\n",
    "            'classifier__n_estimators': [25, 75],\n",
    "            'classifier__max_depth': [7, 12],\n",
    "            'classifier__min_samples_split': [2, 5],\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'mlp': (\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classifier', MLPClassifier(random_state=42, max_iter=500))\n",
    "        ]),\n",
    "        {\n",
    "            'pca': [PCA(100), PCA(200)],\n",
    "            'classifier__hidden_layer_sizes': [(100,), (100, 50), (200, 100)],\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "            'classifier__learning_rate': ['constant', 'adaptive']\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4595007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dense results from file\n"
     ]
    }
   ],
   "source": [
    "dense_results_output_path = 'dense_results.pkl'\n",
    "if not os.path.exists(dense_results_output_path):\n",
    "    print(\"Training dense embedding models...\")\n",
    "    dense_results = cv_trainer(\n",
    "        X_train_dense, X_test_dense, y_train_dense, y_test_dense, \n",
    "        cv=3, models=models_dense # type: ignore\n",
    "    )\n",
    "    joblib.dump(dense_results, dense_results_output_path)\n",
    "else:\n",
    "    dense_results = joblib.load(dense_results_output_path)\n",
    "    print(\"Loaded dense results from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ab90d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: random_forest params: {'classifier__max_depth': 12, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 75, 'pca': None, 'scaler': None}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78      1107\n",
      "           1       0.83      0.64      0.72      1032\n",
      "           2       0.50      0.86      0.64       176\n",
      "\n",
      "    accuracy                           0.74      2315\n",
      "   macro avg       0.69      0.77      0.71      2315\n",
      "weighted avg       0.76      0.74      0.74      2315\n",
      "\n",
      "[[903 130  74]\n",
      " [300 656  76]\n",
      " [ 18   6 152]]\n"
     ]
    }
   ],
   "source": [
    "dense_results = joblib.load(dense_results_output_path)\n",
    "best_dense_model = dense_results['random_forest']['best_estimator']\n",
    "print(f\"best model: {dense_results['summary']['best_model_name']} params: {dense_results['random_forest']['best_params']}\")\n",
    "y_pred_dense = best_dense_model.predict(X_test_dense)\n",
    "print(classification_report(y_test_dense, y_pred_dense))\n",
    "print(confusion_matrix(y_test_dense, y_pred_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5465d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 9/9 [00:56<00:00,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test model performance on scraped digikala comments: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.63      0.45       507\n",
      "           1       0.89      0.59      0.71      1834\n",
      "           2       0.08      0.19      0.11       154\n",
      "\n",
      "    accuracy                           0.57      2495\n",
      "   macro avg       0.44      0.47      0.43      2495\n",
      "weighted avg       0.73      0.57      0.62      2495\n",
      "\n",
      "[[ 320   92   95]\n",
      " [ 504 1082  248]\n",
      " [  76   48   30]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "digikala_scraped_comments_df = pd.read_csv('./data/incredible_offers_product_comments_finalized_labels.csv', index_col=None)\n",
    "digikala_scraped_comments_embeddings = create_embeddings_batch(digikala_scraped_comments_df['text'])\n",
    "print(\"Test model performance on scraped digikala comments: \")\n",
    "y_pred_digikala_dense = best_dense_model.predict(digikala_scraped_comments_embeddings)\n",
    "print(classification_report(digikala_scraped_comments_df['label'].to_list(), y_pred_digikala_dense))\n",
    "print(confusion_matrix(digikala_scraped_comments_df['label'].to_list(), y_pred_digikala_dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e3271",
   "metadata": {},
   "source": [
    "# Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d840d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_df)\n",
    "X_discrete, y_discrete = df[\"text\"].to_numpy(), df[\"label\"].to_numpy()\n",
    "\n",
    "X_discrete_processed = [persian_text_preprocessing(text, stem=True) for text in X_discrete]\n",
    "X_discrete_processed = np.array(X_discrete_processed)\n",
    "\n",
    "X_train_discrete = X_discrete_processed[:train_size]\n",
    "X_test_discrete = X_discrete_processed[train_size:]\n",
    "y_train_discrete, y_test_discrete = y_discrete[:train_size], y_discrete[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "248cf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_stop_words = [\n",
    "    # Articles and determiners\n",
    "    'این', 'آن', 'یک', 'هر', 'همه', 'تمام', 'کل', 'چند', 'بعض', 'برخی',\n",
    "    \n",
    "    # Pronouns\n",
    "    'من', 'تو', 'او', 'ما', 'شما', 'آنها', 'خود', 'خودم', 'خودت', 'خودش',\n",
    "    \n",
    "    # Prepositions (neutral ones)\n",
    "    'در', 'به', 'از', 'با', 'تا', 'روی', 'زیر', 'کنار', 'داخل', 'خارج',\n",
    "    \n",
    "    # Simple conjunctions (not contrastive)\n",
    "    'و', 'یا', 'که', 'چون', 'وقتی', 'زمانی', 'هنگامی',\n",
    "    \n",
    "    # Neutral verbs\n",
    "    'می', 'شود', 'کند', 'دهد', 'است', 'باشد' ,'است', 'بود', 'شد', 'کرد', 'داشت', 'دارد', 'خواهد', 'باید', 'می‌شود',\n",
    "    \n",
    "    # Object markers and particles\n",
    "    'را', 'رو', 'های', 'ها', 'ان', 'ات', 'تان', 'شان',\n",
    "    \n",
    "    # Time references (neutral)\n",
    "    'امروز', 'دیروز', 'فردا', 'حالا', 'الان', 'وقت', 'زمان',\n",
    "    \n",
    "    # Place references\n",
    "    'اینجا', 'آنجا', 'کجا', 'جا', 'محل', 'مکان',\n",
    "    \n",
    "    # Common neutral words\n",
    "    'چیز', 'کار', 'راه', 'نوع', 'قسم', 'طور', 'مثل', 'مانند', 'نام', 'اسم'\n",
    "]\n",
    "\n",
    "models_discrete = {\n",
    "    'logistic_tfidf': (\n",
    "        Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()), \n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('logisticClassifier', LogisticRegression())\n",
    "        ]),\n",
    "        {\n",
    "            'vectorizer__stop_words': [None, persian_stop_words],\n",
    "            'vectorizer__max_features': [500, 2000, None],\n",
    "            'scaler': [None, StandardScaler(with_mean=False)],\n",
    "            'pca': [None, PCA(50), PCA(100)],\n",
    "            'logisticClassifier__C': [0.01, 0.1, 1],\n",
    "            'logisticClassifier__penalty': ['l2'],\n",
    "            'logisticClassifier__max_iter': [1000], \n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'naive_bayes_count': (\n",
    "        Pipeline([\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ]),\n",
    "        {\n",
    "            'vectorizer__stop_words': [None, persian_stop_words],\n",
    "            'vectorizer__max_features': [500, 1000, 2000],\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__min_df': [1, 2],\n",
    "            'classifier__alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'random_forest_tfidf': (\n",
    "        Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        {\n",
    "            'vectorizer__stop_words': [None, persian_stop_words],\n",
    "            'vectorizer__max_features': [1000, 2000],\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__max_depth': [10, 20, None],\n",
    "        }\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb6e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded discrete results from file\n"
     ]
    }
   ],
   "source": [
    "discrete_results_output_path = 'discrete_results.pkl'\n",
    "if not os.path.exists(discrete_results_output_path):\n",
    "    print(\"Training discrete embedding models...\")\n",
    "    discrete_results = cv_trainer(\n",
    "        X_train_discrete, X_test_discrete, y_train_discrete, y_test_discrete, \n",
    "        cv=3, models=models_discrete # type: ignore\n",
    "    )\n",
    "    joblib.dump(discrete_results, discrete_results_output_path)\n",
    "else:\n",
    "    discrete_results = joblib.load(discrete_results_output_path)\n",
    "    print(\"Loaded discrete results from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ad5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: random_forest_tfidf params: {'classifier__max_depth': None, 'classifier__n_estimators': 200, 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 2), 'vectorizer__stop_words': None}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      1107\n",
      "           1       0.92      0.79      0.85      1032\n",
      "           2       0.52      0.98      0.68       176\n",
      "\n",
      "    accuracy                           0.85      2315\n",
      "   macro avg       0.78      0.88      0.80      2315\n",
      "weighted avg       0.87      0.85      0.85      2315\n",
      "\n",
      "[[973  65  69]\n",
      " [132 812  88]\n",
      " [  3   1 172]]\n"
     ]
    }
   ],
   "source": [
    "best_model_name = discrete_results['summary']['best_model_name']\n",
    "best_discrete_model = discrete_results[best_model_name]['best_estimator']\n",
    "print(f\"best model: {discrete_results['summary']['best_model_name']} params: {discrete_results['random_forest_tfidf']['best_params']}\")\n",
    "y_pred_discrete = best_discrete_model.predict(X_test_discrete)\n",
    "print(classification_report(y_test_discrete, y_pred_discrete))\n",
    "print(confusion_matrix(y_test_discrete, y_pred_discrete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d49f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     feature  importance\n",
      "1157     عال    0.024402\n",
      "441      بود    0.019684\n",
      "117     اصلا    0.019306\n",
      "1161   عالیه    0.013981\n",
      "693      خوب    0.012226\n",
      "730      خیل    0.010019\n",
      "1679      ول    0.009407\n",
      "1372   ممنون    0.008116\n",
      "896      راض    0.007931\n",
      "704     خوبه    0.007582\n"
     ]
    }
   ],
   "source": [
    "feature_names = best_discrete_model.named_steps['vectorizer'].get_feature_names_out()\n",
    "feature_importances = best_discrete_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c5ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test model performance on scraped digikala comments: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.57      0.57       507\n",
      "           1       0.93      0.64      0.76      1834\n",
      "           2       0.11      0.54      0.19       154\n",
      "\n",
      "    accuracy                           0.62      2495\n",
      "   macro avg       0.54      0.58      0.50      2495\n",
      "weighted avg       0.81      0.62      0.68      2495\n",
      "\n",
      "[[ 289   64  154]\n",
      " [ 173 1166  495]\n",
      " [  50   21   83]]\n"
     ]
    }
   ],
   "source": [
    "digikala_scraped_comments_df = pd.read_csv('./data/incredible_offers_product_comments_finalized_labels.csv', index_col=None)\n",
    "print(\"Test model performance on scraped digikala comments: \")\n",
    "y_pred_digikala_dense = best_discrete_model.predict(digikala_scraped_comments_df['text'])\n",
    "print(classification_report(digikala_scraped_comments_df['label'].to_list(), y_pred_digikala_dense))\n",
    "print(confusion_matrix(digikala_scraped_comments_df['label'].to_list(), y_pred_digikala_dense))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
