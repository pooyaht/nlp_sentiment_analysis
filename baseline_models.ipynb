{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36e1d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from utils import cv_trainer\n",
    "from base import persian_text_preprocessing\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1762e180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since hezarai/sentiment-dksf couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/tobysmith/.cache/huggingface/datasets/hezarai___sentiment-dksf/default/0.0.0/b4d5a8dd501db610b5ad89e9aa13f863b842b395 (last modified on Sun Jun 29 12:59:27 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d731278ca71944359f13592cbb7ebaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28602 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273fa9e12de4450884e596a066b3eda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"hezarai/sentiment-dksf\")\n",
    "ds = ds.map(lambda example: {**example, 'text': persian_text_preprocessing(example['text'])}, \n",
    "           batched=False)\n",
    "train_df: pd.DataFrame = ds[\"train\"].to_pandas() # type: ignore\n",
    "test_df: pd.DataFrame = ds[\"test\"].to_pandas() # type: ignore\n",
    "df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281f1b6",
   "metadata": {},
   "source": [
    "# Dense Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1cf1e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings from comments_embeddings.npy\n",
      "Embeddings shape: (30917, 768)\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings_batch(texts, model=\"nomic-embed-text\", batch_size=300):\n",
    "    all_embeddings = []\n",
    "    texts_list = texts if isinstance(texts, list) else texts.to_list()\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts_list), batch_size), desc=\"Creating embeddings\"):\n",
    "        batch = texts_list[i:i + batch_size]\n",
    "        response = ollama.embed(model=model, input=batch)\n",
    "        all_embeddings.extend(response[\"embeddings\"])\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "embeddings_file = 'comments_embeddings.npy'\n",
    "\n",
    "if os.path.exists(embeddings_file):\n",
    "    print(f\"Loading existing embeddings from {embeddings_file}\")\n",
    "    comments_dense_embeddings = np.load(embeddings_file)\n",
    "else:\n",
    "    print(\"Creating new embeddings...\")\n",
    "    comments_dense_embeddings = create_embeddings_batch(df[\"text\"])\n",
    "    \n",
    "    np.save(embeddings_file, comments_dense_embeddings)\n",
    "    print(f\"Embeddings saved to {embeddings_file}\")\n",
    "\n",
    "print(f\"Embeddings shape: {comments_dense_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444e23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dense, y_dense = comments_dense_embeddings, df[\"label\"].to_numpy()\n",
    "\n",
    "train_size = len(train_df)\n",
    "X_train_dense = X_dense[:train_size]\n",
    "X_test_dense = X_dense[train_size:]\n",
    "y_train_dense = y_dense[:train_size]\n",
    "y_test_dense = y_dense[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89ba155",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dense = {\n",
    "     'logistic_regression': (\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "        ]),\n",
    "        {\n",
    "            'scaler': [None, StandardScaler()],\n",
    "            'pca': [None, PCA(50), PCA(100), PCA(200)],\n",
    "            'classifier__C': [0.1, 1.0, 10.0],\n",
    "            'classifier__solver': ['liblinear', 'lbfgs'],\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'gaussianNB': (\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classifier', GaussianNB())\n",
    "        ]),\n",
    "        {\n",
    "            'scaler': [None, StandardScaler(), MinMaxScaler()],\n",
    "            'pca': [None, PCA(50), PCA(100), PCA(200)],\n",
    "            'classifier__var_smoothing': [1e-5, 1e-7, 1e-9]\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'random_forest': (\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        {\n",
    "            'scaler': [None, StandardScaler()],\n",
    "            'pca': [None, PCA(100), PCA(200)],\n",
    "            'classifier__n_estimators': [25, 75],\n",
    "            'classifier__max_depth': [7, 12],\n",
    "            'classifier__min_samples_split': [2, 5],\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'mlp': (\n",
    "        Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classifier', MLPClassifier(random_state=42, max_iter=500))\n",
    "        ]),\n",
    "        {\n",
    "            'pca': [PCA(100), PCA(200)],\n",
    "            'classifier__hidden_layer_sizes': [(100,), (100, 50), (200, 100)],\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01],\n",
    "            'classifier__learning_rate': ['constant', 'adaptive']\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4595007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dense results from file\n"
     ]
    }
   ],
   "source": [
    "dense_results_output_path = 'dense_results.pkl'\n",
    "if not os.path.exists(dense_results_output_path):\n",
    "    print(\"Training dense embedding models...\")\n",
    "    dense_results = cv_trainer(\n",
    "        X_train_dense, X_test_dense, y_train_dense, y_test_dense, \n",
    "        cv=3, models=models_dense # type: ignore\n",
    "    )\n",
    "    joblib.dump(dense_results, dense_results_output_path)\n",
    "else:\n",
    "    dense_results = joblib.load(dense_results_output_path)\n",
    "    print(\"Loaded dense results from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ab90d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: random_forest params: {'classifier__max_depth': 12, 'classifier__min_samples_split': 5, 'classifier__n_estimators': 75, 'pca': None, 'scaler': None}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.82      0.78      1107\n",
      "           1       0.83      0.64      0.72      1032\n",
      "           2       0.50      0.86      0.64       176\n",
      "\n",
      "    accuracy                           0.74      2315\n",
      "   macro avg       0.69      0.77      0.71      2315\n",
      "weighted avg       0.76      0.74      0.74      2315\n",
      "\n",
      "[[903 130  74]\n",
      " [300 656  76]\n",
      " [ 18   6 152]]\n"
     ]
    }
   ],
   "source": [
    "dense_results = joblib.load(dense_results_output_path)\n",
    "best_dense_model = dense_results['random_forest']['best_estimator']\n",
    "print(f\"best model: {dense_results['summary']['best_model_name']} params: {dense_results['random_forest']['best_params']}\")\n",
    "y_pred_dense = best_dense_model.predict(X_test_dense)\n",
    "print(classification_report(y_test_dense, y_pred_dense))\n",
    "print(confusion_matrix(y_test_dense, y_pred_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5465d4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|██████████| 9/9 [00:56<00:00,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test model performance on scraped digikala comments: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.63      0.45       507\n",
      "           1       0.89      0.59      0.71      1834\n",
      "           2       0.08      0.19      0.11       154\n",
      "\n",
      "    accuracy                           0.57      2495\n",
      "   macro avg       0.44      0.47      0.43      2495\n",
      "weighted avg       0.73      0.57      0.62      2495\n",
      "\n",
      "[[ 320   92   95]\n",
      " [ 504 1082  248]\n",
      " [  76   48   30]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "digikala_scraped_comments_df = pd.read_csv('incredible_offers_product_comments_finalized_labels.csv', index_col=None)\n",
    "digikala_scraped_comments_embeddings = create_embeddings_batch(digikala_scraped_comments_df['text'])\n",
    "print(\"Test model performance on scraped digikala comments: \")\n",
    "y_pred_digikala_dense = best_dense_model.predict(digikala_scraped_comments_embeddings)\n",
    "print(classification_report(digikala_scraped_comments_df['label'].to_list(), y_pred_digikala_dense))\n",
    "print(confusion_matrix(digikala_scraped_comments_df['label'].to_list(), y_pred_digikala_dense))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e3271",
   "metadata": {},
   "source": [
    "# Discrete Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d840d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_df)\n",
    "X_discrete, y_discrete = df[\"text\"].to_numpy(), df[\"label\"].to_numpy()\n",
    "X_train_discrete, X_test_discrete, y_train_discrete, y_test_discrete = X_discrete[:train_size], X_discrete[train_size:], y_discrete[:train_size], y_discrete[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "248cf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "persian_stop_words = [\n",
    "    # Articles and determiners\n",
    "    'این', 'آن', 'یک', 'هر', 'همه', 'تمام', 'کل', 'چند', 'بعض', 'برخی',\n",
    "    \n",
    "    # Pronouns\n",
    "    'من', 'تو', 'او', 'ما', 'شما', 'آنها', 'خود', 'خودم', 'خودت', 'خودش',\n",
    "    \n",
    "    # Prepositions (neutral ones)\n",
    "    'در', 'به', 'از', 'با', 'تا', 'روی', 'زیر', 'کنار', 'داخل', 'خارج',\n",
    "    \n",
    "    # Simple conjunctions (not contrastive)\n",
    "    'و', 'یا', 'که', 'چون', 'وقتی', 'زمانی', 'هنگامی',\n",
    "    \n",
    "    # Neutral verbs\n",
    "    'می', 'شود', 'کند', 'دهد', 'است', 'باشد' ,'است', 'بود', 'شد', 'کرد', 'داشت', 'دارد', 'خواهد', 'باید', 'می‌شود',\n",
    "    \n",
    "    # Object markers and particles\n",
    "    'را', 'رو', 'های', 'ها', 'ان', 'ات', 'تان', 'شان',\n",
    "    \n",
    "    # Time references (neutral)\n",
    "    'امروز', 'دیروز', 'فردا', 'حالا', 'الان', 'وقت', 'زمان',\n",
    "    \n",
    "    # Place references\n",
    "    'اینجا', 'آنجا', 'کجا', 'جا', 'محل', 'مکان',\n",
    "    \n",
    "    # Common neutral words\n",
    "    'چیز', 'کار', 'راه', 'نوع', 'قسم', 'طور', 'مثل', 'مانند', 'نام', 'اسم'\n",
    "]\n",
    "\n",
    "models_discrete = {\n",
    "    'logistic_tfidf': (\n",
    "        Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()), \n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('logisticClassifier', LogisticRegression())\n",
    "        ]),\n",
    "        {\n",
    "            'vectorizer__stop_words': [None, persian_stop_words],\n",
    "            'vectorizer__max_features': [500, 2000, None],\n",
    "            'scaler': [None, StandardScaler(with_mean=False)],\n",
    "            'pca': [None, PCA(50), PCA(100)],\n",
    "            'logisticClassifier__C': [0.01, 0.1, 1, 10],\n",
    "            'logisticClassifier__penalty': ['l2'],\n",
    "            'logisticClassifier__max_iter': [1000], \n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'naive_bayes_count': (\n",
    "        Pipeline([\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ]),\n",
    "        {\n",
    "            'vectorizer__stop_words': [None, persian_stop_words],\n",
    "            'vectorizer__max_features': [500, 1000, 2000],\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'vectorizer__min_df': [1, 2],\n",
    "            'classifier__alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'random_forest_tfidf': (\n",
    "        Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        {\n",
    "            'vectorizer__stop_words': [None, persian_stop_words],\n",
    "            'vectorizer__max_features': [1000, 2000],\n",
    "            'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__max_depth': [10, 20, None],\n",
    "        }\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb6e139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded discrete results from file\n"
     ]
    }
   ],
   "source": [
    "discrete_results_output_path = 'discrete_results.pkl'\n",
    "if not os.path.exists(discrete_results_output_path):\n",
    "    print(\"Training discrete embedding models...\")\n",
    "    discrete_results = cv_trainer(\n",
    "        X_train_discrete, X_test_discrete, y_train_discrete, y_test_discrete, \n",
    "        cv=3, models=models_discrete # type: ignore\n",
    "    )\n",
    "    joblib.dump(discrete_results, discrete_results_output_path)\n",
    "else:\n",
    "    discrete_results = joblib.load(discrete_results_output_path)\n",
    "    print(\"Loaded discrete results from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61ad5f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: random_forest_tfidf params: {'classifier__max_depth': None, 'classifier__n_estimators': 200, 'vectorizer__max_features': 2000, 'vectorizer__ngram_range': (1, 2), 'vectorizer__stop_words': None}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      1107\n",
      "           1       0.93      0.79      0.86      1032\n",
      "           2       0.51      0.98      0.68       176\n",
      "\n",
      "    accuracy                           0.85      2315\n",
      "   macro avg       0.78      0.88      0.80      2315\n",
      "weighted avg       0.88      0.85      0.85      2315\n",
      "\n",
      "[[975  57  75]\n",
      " [128 816  88]\n",
      " [  2   1 173]]\n"
     ]
    }
   ],
   "source": [
    "best_model_name = discrete_results['summary']['best_model_name']\n",
    "best_discrete_model = discrete_results[best_model_name]['best_estimator']\n",
    "print(f\"best model: {discrete_results['summary']['best_model_name']} params: {discrete_results['random_forest_tfidf']['best_params']}\")\n",
    "y_pred_discrete = best_discrete_model.predict(X_test_discrete)\n",
    "print(classification_report(y_test_discrete, y_pred_discrete))\n",
    "print(confusion_matrix(y_test_discrete, y_pred_discrete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56d49f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  importance\n",
      "1151      عالی    0.026685\n",
      "115       اصلا    0.019080\n",
      "428        بود    0.018924\n",
      "1154     عالیه    0.013610\n",
      "685        خوب    0.010724\n",
      "724       خیلی    0.009948\n",
      "1681       ولی    0.009054\n",
      "721     خوشمزه    0.007562\n",
      "689       خوبه    0.007491\n",
      "1152  عالی بود    0.007170\n"
     ]
    }
   ],
   "source": [
    "feature_names = best_discrete_model.named_steps['vectorizer'].get_feature_names_out()\n",
    "feature_importances = best_discrete_model.named_steps['classifier'].feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95c5ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test model performance on scraped digikala comments: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.60      0.61       507\n",
      "           1       0.94      0.75      0.83      1834\n",
      "           2       0.15      0.56      0.24       154\n",
      "\n",
      "    accuracy                           0.70      2495\n",
      "   macro avg       0.58      0.63      0.56      2495\n",
      "weighted avg       0.83      0.70      0.75      2495\n",
      "\n",
      "[[ 304   57  146]\n",
      " [ 137 1367  330]\n",
      " [  42   26   86]]\n"
     ]
    }
   ],
   "source": [
    "digikala_scraped_comments_df = pd.read_csv('incredible_offers_product_comments_finalized_labels.csv', index_col=None)\n",
    "print(\"Test model performance on scraped digikala comments: \")\n",
    "y_pred_digikala_dense = best_discrete_model.predict(digikala_scraped_comments_df['text'])\n",
    "print(classification_report(digikala_scraped_comments_df['label'].to_list(), y_pred_digikala_dense))\n",
    "print(confusion_matrix(digikala_scraped_comments_df['label'].to_list(), y_pred_digikala_dense))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
